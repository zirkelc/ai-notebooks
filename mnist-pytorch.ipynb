{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-09T06:30:32.388025Z","iopub.execute_input":"2024-04-09T06:30:32.388467Z","iopub.status.idle":"2024-04-09T06:30:32.399471Z","shell.execute_reply.started":"2024-04-09T06:30:32.388433Z","shell.execute_reply":"2024-04-09T06:30:32.398252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"digits = {\n    0: [[0,0,0,0,0,0,0],\n        [0,1,1,1,1,1,0],\n        [0,1,0,0,0,1,0],\n        [0,1,0,0,0,1,0],\n        [0,1,0,0,0,1,0],\n        [0,1,1,1,1,1,0],\n        [0,0,0,0,0,0,0]],\n    \n    1: [[0,0,0,0,0,0,0],\n        [0,0,0,1,0,0,0],\n        [0,0,1,1,0,0,0],\n        [0,0,0,1,0,0,0],\n        [0,0,0,1,0,0,0],\n        [0,0,0,1,0,0,0],\n        [0,0,0,0,0,0,0]],\n    \n    2: [[0,0,0,0,0,0,0],\n        [0,1,1,1,1,1,0],\n        [0,0,0,0,0,1,0],\n        [0,1,1,1,1,1,0],\n        [0,1,0,0,0,0,0],\n        [0,1,1,1,1,1,0],\n        [0,0,0,0,0,0,0]],\n    \n    3: [[0,0,0,0,0,0,0],\n        [0,1,1,1,1,1,0],\n        [0,0,0,0,0,1,0],\n        [0,0,1,1,1,1,0],\n        [0,0,0,0,0,1,0],\n        [0,1,1,1,1,1,0],\n        [0,0,0,0,0,0,0]],\n    \n    4: [[0,0,0,0,0,0,0],\n        [0,0,1,0,1,0,0],\n        [0,0,1,0,1,0,0],\n        [0,0,1,1,1,1,0],\n        [0,0,0,0,1,0,0],\n        [0,0,0,0,1,0,0],\n        [0,0,0,0,0,0,0]],\n    \n    5: [[0,0,0,0,0,0,0],\n        [0,1,1,1,1,1,0],\n        [0,1,0,0,0,0,0],\n        [0,1,1,1,1,1,0],\n        [0,0,0,0,0,1,0],\n        [0,1,1,1,1,1,0],\n        [0,0,0,0,0,0,0]],\n    \n    6: [[0,0,0,0,0,0,0],\n        [0,0,1,1,1,1,0],\n        [0,1,0,0,0,0,0],\n        [0,1,1,1,1,1,0],\n        [0,1,0,0,0,1,0],\n        [0,1,1,1,1,1,0],\n        [0,0,0,0,0,0,0]],\n    \n    7: [[0,0,0,0,0,0,0],\n        [0,1,1,1,1,1,0],\n        [0,0,0,0,0,1,0],\n        [0,0,0,0,1,0,0],\n        [0,0,0,1,0,0,0],\n        [0,0,1,0,0,0,0],\n        [0,0,0,0,0,0,0]],\n    \n    8: [[0,0,1,1,1,0,0],\n        [0,1,0,0,0,1,0],\n        [0,1,0,0,0,1,0],\n        [0,0,1,1,1,0,0],\n        [0,1,0,0,0,1,0],\n        [0,1,0,0,0,1,0],\n        [0,0,1,1,1,0,0]],\n\n    9: [[0,0,1,1,1,0,0],\n        [0,1,0,0,0,1,0],\n        [0,1,0,0,0,1,0],\n        [0,0,1,1,1,1,0],\n        [0,0,0,0,0,1,0],\n        [0,1,0,0,0,1,0],\n        [0,0,1,1,1,0,0]],\n}\n\nfor i in digits:\n    # https://github.com/geohot/ai-notebooks/blob/master/mnist_from_scratch.ipynb\n    m = np.concatenate([np.concatenate([[x]*4 for x in y]*4) for y in digits[i]])\n    digits[i] = m.reshape(28,28)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:32.402132Z","iopub.execute_input":"2024-04-09T06:30:32.402629Z","iopub.status.idle":"2024-04-09T06:30:32.441054Z","shell.execute_reply.started":"2024-04-09T06:30:32.402585Z","shell.execute_reply":"2024-04-09T06:30:32.439632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load train and test datasets\ntrain_df = pd.read_csv(\"../input/digit-recognizer/train.csv\")\ntest_df = pd.read_csv(\"../input/digit-recognizer/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:32.442776Z","iopub.execute_input":"2024-04-09T06:30:32.443507Z","iopub.status.idle":"2024-04-09T06:30:37.577075Z","shell.execute_reply.started":"2024-04-09T06:30:32.443472Z","shell.execute_reply":"2024-04-09T06:30:37.575777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:37.579770Z","iopub.execute_input":"2024-04-09T06:30:37.580179Z","iopub.status.idle":"2024-04-09T06:30:37.601464Z","shell.execute_reply.started":"2024-04-09T06:30:37.580144Z","shell.execute_reply":"2024-04-09T06:30:37.600457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:37.602807Z","iopub.execute_input":"2024-04-09T06:30:37.603603Z","iopub.status.idle":"2024-04-09T06:30:37.630007Z","shell.execute_reply.started":"2024-04-09T06:30:37.603567Z","shell.execute_reply":"2024-04-09T06:30:37.628773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if train and test dataset have the same number of columns\n# Exclude first column `label` from the comparison\nassert train_df.iloc[:, 1:].columns.equals(test_df.columns), \"train_df and test_df do not have the same columns\"\n\nprint(f\"train_df.shape: {train_df.shape}\")\nprint(f\"test_df.shape: {test_df.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:37.631823Z","iopub.execute_input":"2024-04-09T06:30:37.632211Z","iopub.status.idle":"2024-04-09T06:30:37.650832Z","shell.execute_reply.started":"2024-04-09T06:30:37.632178Z","shell.execute_reply":"2024-04-09T06:30:37.649357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_split(X, y, test_size=0.1):\n    # Ensure that X and y are numpy arrays. If they're not, convert them.\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Create a range from 0 to the length of X rows\n    indices = np.arange(X.shape[0])\n    # Suffle indices\n    np.random.shuffle(indices)\n    \n    # Determine the split index\n    split_idx = int(X.shape[0] * (1 - test_size))\n    \n    # Split the indices into training and testing parts\n    train_idx, test_idx = indices[:split_idx], indices[split_idx:]\n    \n    # Split the data and labels into training and testing sets\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    return X_train, X_test, y_train, y_test","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:37.655376Z","iopub.execute_input":"2024-04-09T06:30:37.655864Z","iopub.status.idle":"2024-04-09T06:30:37.666108Z","shell.execute_reply.started":"2024-04-09T06:30:37.655828Z","shell.execute_reply":"2024-04-09T06:30:37.664828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the training dataset into training and validation datasets\ny = train_df.iloc[:, 0]  # Select all rows of the first column (label column)\nX = train_df.iloc[:, 1:] # Select all rows and all columns except the first one\n\nX_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.1)\n\nprint(f\"training dataset: {X_train.shape}\")\nprint(f\"validation dataset: {X_validate.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:37.667444Z","iopub.execute_input":"2024-04-09T06:30:37.667876Z","iopub.status.idle":"2024-04-09T06:30:38.307902Z","shell.execute_reply.started":"2024-04-09T06:30:37.667844Z","shell.execute_reply":"2024-04-09T06:30:38.306568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ndef build_model(nodes=[128]):\n    model = nn.Sequential(\n        nn.Linear(784, 256),\n        nn.ReLU(),\n        nn.Linear(256, 10)\n    )\n\n    criterion = nn.CrossEntropyLoss()\n    #optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:38.309430Z","iopub.execute_input":"2024-04-09T06:30:38.309835Z","iopub.status.idle":"2024-04-09T06:30:38.319536Z","shell.execute_reply.started":"2024-04-09T06:30:38.309803Z","shell.execute_reply":"2024-04-09T06:30:38.318417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_batches_numpy(data, labels, batch_size):\n    indices = np.arange(len(data))\n    np.random.shuffle(indices)\n    \n    num_batches = len(data) // batch_size\n    \n    for batch_idx in range(num_batches):\n        start_idx = batch_idx * batch_size\n        end_idx   = start_idx + batch_size\n        batch     = indices[start_idx:end_idx]\n        yield data[batch], labels[batch]","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:38.324886Z","iopub.execute_input":"2024-04-09T06:30:38.325383Z","iopub.status.idle":"2024-04-09T06:30:38.333304Z","shell.execute_reply.started":"2024-04-09T06:30:38.325348Z","shell.execute_reply":"2024-04-09T06:30:38.331795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\n\ndef get_batches_pytorch(data, labels, batch_size):\n    dataset = TensorDataset(torch.tensor(data, dtype=torch.float), torch.tensor(labels))\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:38.334705Z","iopub.execute_input":"2024-04-09T06:30:38.335101Z","iopub.status.idle":"2024-04-09T06:30:38.349670Z","shell.execute_reply.started":"2024-04-09T06:30:38.335068Z","shell.execute_reply":"2024-04-09T06:30:38.348455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def forward(inputs, labels, train=False):\n    inputs = inputs if torch.is_tensor(inputs) else torch.tensor(inputs, dtype=torch.float)\n    labels = labels if torch.is_tensor(labels) else torch.tensor(labels)\n    \n    # Forward pass with tracking gradients\n    if train:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        predictions = torch.argmax(outputs, dim=1)\n        loss.backward()\n        optimizer.step()   \n        \n        return loss.item(), predictions\n    \n    # Forward pass without tracking gradients\n    with torch.no_grad():\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        predictions = torch.argmax(outputs, dim=1)\n\n        return loss.item(), predictions","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:38.351357Z","iopub.execute_input":"2024-04-09T06:30:38.351785Z","iopub.status.idle":"2024-04-09T06:30:38.365333Z","shell.execute_reply.started":"2024-04-09T06:30:38.351744Z","shell.execute_reply":"2024-04-09T06:30:38.364173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(inputs):\n    # Forward pass without tracking gradients\n    with torch.no_grad():\n        inputs = inputs if torch.is_tensor(inputs) else torch.tensor(inputs, dtype=torch.float)\n        \n        outputs = model(inputs)        \n        probabilities = F.softmax(outputs, dim=1)\n        predictions = torch.argmax(outputs, dim=1)\n        \n        return predictions.numpy()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:38.366837Z","iopub.execute_input":"2024-04-09T06:30:38.367389Z","iopub.status.idle":"2024-04-09T06:30:38.379438Z","shell.execute_reply.started":"2024-04-09T06:30:38.367333Z","shell.execute_reply":"2024-04-09T06:30:38.378227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(labels, predictions):\n    labels = labels if torch.is_tensor(labels) else torch.tensor(labels)\n    \n    # Calculate the number of correct predictions\n    correct_predictions = torch.sum(predictions == labels)\n\n    # To calculate accuracy\n    accuracy = correct_predictions.item() / labels.size(0)\n    \n    return accuracy*100","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:38.384259Z","iopub.execute_input":"2024-04-09T06:30:38.384655Z","iopub.status.idle":"2024-04-09T06:30:38.392849Z","shell.execute_reply.started":"2024-04-09T06:30:38.384621Z","shell.execute_reply":"2024-04-09T06:30:38.391543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TODO:\n* test multiple networks\n* torch data loader\n* test different learning rates\n* test batchnorm\n* visualize activations","metadata":{}},{"cell_type":"code","source":"num_epochs=1000\nbatch_size=128\n\n# Calculate number of batches\nnum_batches = X_train.shape[0] // batch_size\n\n# Track losses per epoch\ntrain_losses = [0] * num_epochs\ntrain_accuracy = [0] * num_epochs\nvalidate_losses = [0] * num_epochs\nvalidate_accuracy = [0] * num_epochs\n\n# DataLoader\n# numpy: get_batches_numpy(X_train, y_train, batch_size):\ndataloader = get_batches_pytorch(X_train, y_train, batch_size)\n\nprint(f\"Training on {len(X_train)} samples\")\nprint(f\"Epochs: {num_epochs}\")\nprint(f\"Batches: {num_batches} with size {batch_size}\")\n\ninitial_loss, _ = forward(X_train, y_train, False)\nprint(f\"Initial loss: {initial_loss:.3f}\")\n\nprint(\"Training started.\")\n\nfor epoch in range(num_epochs):\n    for X_batch, y_batch in dataloader:\n        forward(X_batch, y_batch, True)\n    \n    train_loss, train_prediction = forward(X_train, y_train, False)\n    train_losses[epoch] = train_loss\n    train_accuracy[epoch] = accuracy(y_train, train_prediction)\n    \n    validate_loss, validate_prediction = forward(X_validate, y_validate, False)\n    validate_losses[epoch] = validate_loss\n    validate_accuracy[epoch] = accuracy(y_validate, validate_prediction)\n    \n    print(f\"Epoch {epoch}, train loss {train_losses[epoch]:.3f}, validate loss {validate_losses[epoch]:.3f}, accuracy {validate_accuracy[epoch]:.3f}\")\n        \nprint(\"Training completed.\")\nprint(f\"Training loss: {train_losses[-1]:.3f}\")\nprint(f\"Validation loss: {validate_losses[-1]:.3f}\")\nprint(f\"Validation accuracy: {validate_accuracy[-1]:.3f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:38.394333Z","iopub.execute_input":"2024-04-09T06:30:38.394665Z","iopub.status.idle":"2024-04-09T06:30:50.179876Z","shell.execute_reply.started":"2024-04-09T06:30:38.394635Z","shell.execute_reply":"2024-04-09T06:30:50.178485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot the results\nplt.subplot(211)\nplt.ylabel('Accuracy')\nplt.plot(train_accuracy, label='Training Accuracy')\nplt.plot(validate_accuracy, label='Validation Accuracy')\nplt.legend()\n\nplt.subplot(212)\nplt.ylabel('Loss')\nplt.xlabel(\"Epoch\")\nplt.plot(train_losses, label='Training Loss')\nplt.plot(validate_losses, label='Validation Loss')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:50.181911Z","iopub.execute_input":"2024-04-09T06:30:50.182293Z","iopub.status.idle":"2024-04-09T06:30:50.714645Z","shell.execute_reply.started":"2024-04-09T06:30:50.182259Z","shell.execute_reply":"2024-04-09T06:30:50.713358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions\nX_test_digits = np.array([d.flatten() for d in digits.values()])\ny_pred_digits = predict(X_test_digits)\n\nfig, ax = plt.subplots(1, 10, figsize=(10, 10))\n\nfor digit, matrix in digits.items():\n    ax[digit].imshow(matrix, cmap='gray')\n    ax[digit].set_title(str(y_pred_digits[digit]))\n    ax[digit].axis('off')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:50.716512Z","iopub.execute_input":"2024-04-09T06:30:50.717421Z","iopub.status.idle":"2024-04-09T06:30:51.389685Z","shell.execute_reply.started":"2024-04-09T06:30:50.717372Z","shell.execute_reply":"2024-04-09T06:30:51.388351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = test_df.to_numpy()\n\ny_test = predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:51.391592Z","iopub.execute_input":"2024-04-09T06:30:51.392082Z","iopub.status.idle":"2024-04-09T06:30:51.513551Z","shell.execute_reply.started":"2024-04-09T06:30:51.392037Z","shell.execute_reply":"2024-04-09T06:30:51.512486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create submission dataframe\nsubmission_df = pd.read_csv(\"../input/digit-recognizer/sample_submission.csv\")\nsubmission_df[\"Label\"] = y_test\nsubmission_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:51.516562Z","iopub.execute_input":"2024-04-09T06:30:51.517239Z","iopub.status.idle":"2024-04-09T06:30:51.539030Z","shell.execute_reply.started":"2024-04-09T06:30:51.517203Z","shell.execute_reply":"2024-04-09T06:30:51.537773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:30:51.540546Z","iopub.execute_input":"2024-04-09T06:30:51.541622Z","iopub.status.idle":"2024-04-09T06:30:51.587805Z","shell.execute_reply.started":"2024-04-09T06:30:51.541572Z","shell.execute_reply":"2024-04-09T06:30:51.586442Z"},"trusted":true},"execution_count":null,"outputs":[]}]}