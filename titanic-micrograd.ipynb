{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/zirklelc/micrograd?scriptVersionId=156394207\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"###  Titanic with MicroGrad","metadata":{}},{"cell_type":"markdown","source":"# Titanic with Micrograd\nThis notebook uses Andrej Karpathy's [Micrograd](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbkJGdDA2Y3JzZHlPc0lIOU5DdDVMRTc5cldFQXxBQ3Jtc0trN3ZCRGxaYmtXRWhmUm4wVVZHV2pfdWtuUllIOHl0aFdtSGxTNEpkQ2stY25lY2t6bzIxR2tCWHBGZDNJU3FfTk0xcWFQN0dMZGw2TU1UNE9VWXlvY1pBMmZjR0VYZkJYd1ppWTZlN3UzWURNdlZkSQ&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2Fmicrograd&v=VMj-3S1tku0) library to solve the [Titanic challenge](https://www.kaggle.com/competitions/titanic) on Kaggle. I recommend you watch his phenomenal YouTube video on [building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0&t=2s&ab_channel=AndrejKarpathy) for an introduction to neural networks and backpropagation. The notebook is available on [Kaggle](https://www.kaggle.com/code/zirklelc/micrograd) in different versions and with different scores. The current best score of [0.76555 (v9)](https://www.kaggle.com/code/zirklelc/micrograd?scriptVersionId=156800490) is very close to other implementations with PyTorch.\n\n## References\nHere are some references that helped me with data preparation and implementation:\n- https://danielmuellerkomorowska.com/2021/02/03/a-deep-feedforward-network-in-pytorch-for-the-titanic-challenge/\n- https://www.kaggle.com/code/jcardenzana/titanic-pytorch\n- https://www.kaggle.com/code/kiranscaria/titanic-pytorch\n- https://github.com/kurtispykes/Machine-Learning","metadata":{}},{"cell_type":"code","source":"!pip install micrograd","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:45.44597Z","iopub.execute_input":"2023-12-25T07:03:45.446593Z","iopub.status.idle":"2023-12-25T07:03:57.495491Z","shell.execute_reply.started":"2023-12-25T07:03:45.446555Z","shell.execute_reply":"2023-12-25T07:03:57.494267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom micrograd.engine import Value\nfrom micrograd.nn import MLP\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:57.49767Z","iopub.execute_input":"2023-12-25T07:03:57.498766Z","iopub.status.idle":"2023-12-25T07:03:57.848144Z","shell.execute_reply.started":"2023-12-25T07:03:57.498728Z","shell.execute_reply":"2023-12-25T07:03:57.847279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load train and test datasets\ntrain_df = pd.read_csv(\"../input/titanic/train.csv\", index_col=\"PassengerId\")\ntest_df = pd.read_csv(\"../input/titanic/test.csv\", index_col=\"PassengerId\")\n\ntrain_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:57.849425Z","iopub.execute_input":"2023-12-25T07:03:57.849907Z","iopub.status.idle":"2023-12-25T07:03:57.917698Z","shell.execute_reply.started":"2023-12-25T07:03:57.849844Z","shell.execute_reply":"2023-12-25T07:03:57.915834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess datasets\ndatasets = [train_df, test_df]\n\n# Calculate the median age and fare from teh training dataset\nmedian_age = train_df[\"Age\"].median()\nmedian_fare = train_df[\"Fare\"].median()\n\n# Iterate over both datasets\nfor dataset in datasets:\n    # Fill NaNs for `Age` and `Fare` with the columns' median value.\n    # Note to fill NaNs in the test dataset with the median values from the training dataset.\n    dataset[\"Age\"].fillna(median_age, inplace=True)\n    dataset[\"Fare\"].fillna(median_fare, inplace=True)\n\n    # Convert `Sex` into categorical feature\n    dataset[\"Sex\"] = pd.Categorical(dataset[\"Sex\"])\n    dataset[\"Sex\"] = dataset[\"Sex\"].cat.codes\n\n    # Note not to convert `Embarked` into a categorical feature here, because the training set missing values but the test set does not.\n    # This results in more columns on the training dataset than on the test dataset when converted into categorical features.\n    # This will be handled by the `get_dummies` function later.\n    dataset[\"Embarked\"] = dataset[\"Embarked\"]\n\n    # Drop columns that are not useful for prediction\n    dataset.drop([\"Name\", \"Ticket\", \"Cabin\"], axis=1, inplace=True)\n\ntrain_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:57.921279Z","iopub.execute_input":"2023-12-25T07:03:57.922779Z","iopub.status.idle":"2023-12-25T07:03:57.954926Z","shell.execute_reply.started":"2023-12-25T07:03:57.92273Z","shell.execute_reply":"2023-12-25T07:03:57.953411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check training dataset for data types\ntrain_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:57.9563Z","iopub.execute_input":"2023-12-25T07:03:57.956626Z","iopub.status.idle":"2023-12-25T07:03:57.97764Z","shell.execute_reply.started":"2023-12-25T07:03:57.956596Z","shell.execute_reply":"2023-12-25T07:03:57.975665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check test dataset for data types\ntest_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:57.978619Z","iopub.execute_input":"2023-12-25T07:03:57.97885Z","iopub.status.idle":"2023-12-25T07:03:57.993556Z","shell.execute_reply.started":"2023-12-25T07:03:57.978823Z","shell.execute_reply":"2023-12-25T07:03:57.992347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the columns for one-hot encoding\ncategorical_cols = [\"Pclass\", \"Sex\", \"Embarked\", \"SibSp\"]\n\n# Convert categorical variable into dummy/indicator variables.\n# Note to use the `dummy_na=True` parameter to create a column for unknown values\n# https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html\ntrain_dummies_df = pd.get_dummies(\n    train_df,\n    columns=categorical_cols,\n    prefix=categorical_cols,\n    dummy_na=True,\n    # dtype=int\n)\ntest_dummies_df = pd.get_dummies(\n    test_df,\n    columns=categorical_cols,\n    prefix=categorical_cols,\n    dummy_na=True,\n    # dtype=int\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:57.995474Z","iopub.execute_input":"2023-12-25T07:03:57.9958Z","iopub.status.idle":"2023-12-25T07:03:58.020046Z","shell.execute_reply.started":"2023-12-25T07:03:57.995775Z","shell.execute_reply":"2023-12-25T07:03:58.018212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dummies_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:58.021662Z","iopub.execute_input":"2023-12-25T07:03:58.02207Z","iopub.status.idle":"2023-12-25T07:03:58.048584Z","shell.execute_reply.started":"2023-12-25T07:03:58.022031Z","shell.execute_reply":"2023-12-25T07:03:58.047297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dummies_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:58.049635Z","iopub.execute_input":"2023-12-25T07:03:58.049911Z","iopub.status.idle":"2023-12-25T07:03:58.075339Z","shell.execute_reply.started":"2023-12-25T07:03:58.049881Z","shell.execute_reply":"2023-12-25T07:03:58.074191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if train and test dataset have the same number of columns\n# Note `Survived` is not included in test data set, so we exclude it from the comparison\nassert train_dummies_df.iloc[:, 1:].columns.equals(\n    test_dummies_df.columns\n), \"train_dummies_df and test_dummies_df do not have the same columns\"\n\nprint(f\"train_dummies_df.shape: {train_dummies_df.shape}\")\nprint(f\"test_dummies_df.shape: {test_dummies_df.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:58.079834Z","iopub.execute_input":"2023-12-25T07:03:58.080161Z","iopub.status.idle":"2023-12-25T07:03:58.089729Z","shell.execute_reply.started":"2023-12-25T07:03:58.080137Z","shell.execute_reply":"2023-12-25T07:03:58.088202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate survival rate\ntotal_samples = train_dummies_df.shape[0]  # Total number of samples\nnum_survived = (train_dummies_df[\"Survived\"] == 1).sum()  # Number of survivors\nrate_survival = (num_survived / total_samples) * 100\n\nprint(f\"Survival rate: {rate_survival:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:58.091338Z","iopub.execute_input":"2023-12-25T07:03:58.091839Z","iopub.status.idle":"2023-12-25T07:03:58.10149Z","shell.execute_reply.started":"2023-12-25T07:03:58.091757Z","shell.execute_reply":"2023-12-25T07:03:58.100592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardize datasets\nfrom sklearn.preprocessing import StandardScaler\n\n# Use standard scaling with mean and standard deviation from the training dataset\n# Note to use the same scaler for both training and test datasets\n# The `Survived` column is excluded from the scaling by using `iloc[:, 1:]`\n# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\nscaler = StandardScaler()\nscaler.fit(train_dummies_df.iloc[:, 1:])\ntrain_scaled = scaler.transform(train_dummies_df.iloc[:, 1:])\ntest_scaled = scaler.transform(test_dummies_df)\n\ntrain_scaled_df = pd.DataFrame(\n    train_scaled,\n    index=train_dummies_df.index,\n    columns=train_dummies_df.iloc[:, 1:].columns,\n)\n\ntest_scaled_df = pd.DataFrame(\n    test_scaled, index=test_dummies_df.index, columns=test_dummies_df.columns\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:58.102563Z","iopub.execute_input":"2023-12-25T07:03:58.102903Z","iopub.status.idle":"2023-12-25T07:03:58.504649Z","shell.execute_reply.started":"2023-12-25T07:03:58.102851Z","shell.execute_reply":"2023-12-25T07:03:58.503333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dummies_df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:58.506275Z","iopub.execute_input":"2023-12-25T07:03:58.506578Z","iopub.status.idle":"2023-12-25T07:03:58.532193Z","shell.execute_reply.started":"2023-12-25T07:03:58.506555Z","shell.execute_reply":"2023-12-25T07:03:58.530966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_scaled_df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:58.533661Z","iopub.execute_input":"2023-12-25T07:03:58.533966Z","iopub.status.idle":"2023-12-25T07:03:58.59751Z","shell.execute_reply.started":"2023-12-25T07:03:58.533942Z","shell.execute_reply":"2023-12-25T07:03:58.596632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check train and test datasets have the same columns\n# Note `Surivived` was removed during scaling\nassert train_scaled_df.columns.equals(\n    test_scaled_df.columns\n), \"train_scaled_df and test_scaled_df do not have the same columns\"","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:58.598739Z","iopub.execute_input":"2023-12-25T07:03:58.599028Z","iopub.status.idle":"2023-12-25T07:03:58.606131Z","shell.execute_reply.started":"2023-12-25T07:03:58.599003Z","shell.execute_reply":"2023-12-25T07:03:58.60447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Take target labels from the unscaled training dataset and input features from the scaled training dataset\ntrain_labels = train_dummies_df[\"Survived\"].to_numpy()\ntrain_features = train_scaled_df.to_numpy()\n\n# Split the training dataset into training and validation datasets\n# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\nX_train, X_validate, y_train, y_validate = train_test_split(\n    train_features, train_labels, test_size=0.1\n)\n\nprint(f\"training dataset: {X_train.shape}\")\nprint(f\"validation dataset: {X_validate.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:58.608177Z","iopub.execute_input":"2023-12-25T07:03:58.608681Z","iopub.status.idle":"2023-12-25T07:03:58.801152Z","shell.execute_reply.started":"2023-12-25T07:03:58.608652Z","shell.execute_reply":"2023-12-25T07:03:58.799253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_model(n_input, n_hidden=[], n_output=1):\n    nodes = n_hidden + [n_output]\n\n    model = MLP(n_input, nodes)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:58.802552Z","iopub.execute_input":"2023-12-25T07:03:58.802892Z","iopub.status.idle":"2023-12-25T07:03:58.80847Z","shell.execute_reply.started":"2023-12-25T07:03:58.802841Z","shell.execute_reply":"2023-12-25T07:03:58.806929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\n# Micograd doesn't have a sigmoid function\ndef sigmoid(value):\n    x = value.data\n    e = math.exp(x)\n    t = (e) / (e + 1)\n    out = Value(t, (value,), \"Sigmoid\")\n\n    def _backward():\n            value.grad += (e) / ((1 + e) ** 2) * out.grad\n\n    out._backward = _backward\n\n    return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def forward(model, features):\n    # assert features is 2d numpy array\n    assert len(features.shape) == 2\n\n    # map features to micrograd values\n    inputs = [list(map(Value, row)) for row in features]\n\n    # forward the model to get predictions\n    predictions = list(map(model, inputs))\n\n    # apply sigmoid to predictions\n    predictions = list(map(sigmoid, predictions))\n\n    return np.asarray(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:58.810595Z","iopub.execute_input":"2023-12-25T07:03:58.811067Z","iopub.status.idle":"2023-12-25T07:03:58.821307Z","shell.execute_reply.started":"2023-12-25T07:03:58.811032Z","shell.execute_reply":"2023-12-25T07:03:58.819644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss(predictions, labels):\n    # assert predictions and labels are 1d numpy arrays\n    assert len(predictions.shape) == 1, \"predictions must be 1d numpy array\"\n    assert len(labels.shape) == 1, \"labels must be 1d numpy array\"\n    assert len(predictions) == len(\n        labels\n    ), \"predictions and labels must have the same length\"\n\n    # svm \"max-margin\" loss\n    # losses = [(1 + -label*prediction).relu() for label, prediction in zip(labels, predictions)]\n    # data_loss = sum(losses) * (1.0 / len(losses))\n\n    # L2 regularization\n    # alpha = 1e-4\n    # reg_loss = alpha * sum((p*p for p in model.parameters()))\n    # total_loss = data_loss + reg_loss\n    # return total_loss\n    \n    # mean squared error loss\n    losses = [\n        (prediction - label) ** 2 for label, prediction in zip(labels, predictions)\n    ]\n    total_loss = sum(losses) * (1.0 / len(losses))\n    return total_loss\n","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:58.82391Z","iopub.execute_input":"2023-12-25T07:03:58.824449Z","iopub.status.idle":"2023-12-25T07:03:58.839845Z","shell.execute_reply.started":"2023-12-25T07:03:58.824412Z","shell.execute_reply":"2023-12-25T07:03:58.838283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(predictions, labels):\n    # assert predictions and labels are 1d numpy arrays\n    assert len(predictions.shape) == 1, \"predictions must be 1d numpy array\"\n    assert len(labels.shape) == 1, \"labels must be 1d numpy array\"\n    assert len(predictions) == len(\n            labels\n    ), \"predictions and labels must have the same length\"\n\n    # Extract values from micrograd Value objects\n    predicted_values = np.array([value.data for value in predictions])\n\n    # Convert predictions to binary values (0 or 1) based on the threshold\n    binary_predictions = (predicted_values > 0.5).astype(int)\n\n    # Compare binary_predictions with true_labels\n    correct_predictions = np.sum(binary_predictions == labels)\n\n    # Calculate accuracy\n    accuracy = correct_predictions / len(labels)\n\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:58.841788Z","iopub.execute_input":"2023-12-25T07:03:58.842314Z","iopub.status.idle":"2023-12-25T07:03:58.853169Z","shell.execute_reply.started":"2023-12-25T07:03:58.842282Z","shell.execute_reply":"2023-12-25T07:03:58.851596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def optimize(model, epoch, loss):\n    # unpack epochs\n    epoch, num_epochs = epoch\n    \n    model.zero_grad()\n    loss.backward()\n\n    # learning_rate = 1.0-0.9*k/100\n    # learning_rate = 0.001\n    start_lr = 0.01\n    end_lr = 0.001\n    learning_rate = max(\n        (start_lr - (start_lr - end_lr) * epoch / (num_epochs - 1)), \n        end_lr\n    )\n    for p in model.parameters():\n        p.data -= learning_rate * p.grad","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:58.854621Z","iopub.execute_input":"2023-12-25T07:03:58.855326Z","iopub.status.idle":"2023-12-25T07:03:58.869085Z","shell.execute_reply.started":"2023-12-25T07:03:58.855298Z","shell.execute_reply":"2023-12-25T07:03:58.867768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import shuffle\n\n\ndef train_model(model, xy_train, xy_validate, num_epochs=100, batch_size=32):\n    # Unpack training and validation data\n    x_train, y_train = xy_train\n    x_validate, y_validate = xy_validate\n\n    # Calculate number of batches\n    batch_size = x_train.shape[0] if batch_size == None else batch_size\n    num_batches = x_train.shape[0] // batch_size\n\n    # Losses per epoch\n    train_losses = [0] * num_epochs\n    validate_losses = [0] * num_epochs\n    validate_accuracy = [0] * num_epochs\n\n    print(f\"Training on {x_train.shape[0]} samples\")\n    print(f\"Epochs: {num_epochs}\")\n    print(f\"Batches: {num_batches} with size {batch_size}\")\n\n    for epoch in range(num_epochs):\n        # Shuffle training data at the beginning of each epoch\n        x_train, y_train = shuffle(x_train, y_train)\n\n        for batch in range(num_batches):\n            # Calculate next batch indices\n            start = batch * batch_size\n            end = start + batch_size\n            x_batch, y_batch = x_train[start:end], y_train[start:end]\n\n            # sample a random batch from the training data\n            #ri = np.random.permutation(x_train.shape[0])[:batch_size]\n            #x_batch, y_batch = x_train[ri], y_train[ri]\n\n            # train on batch\n            train_output = forward(model, x_batch)\n            train_loss = loss(train_output, y_batch)\n\n            # optimize after each batch\n            optimize(model, (epoch, num_epochs), train_loss)\n\n        # forward full training set\n        train_output = forward(model, x_train)\n        train_loss = loss(train_output, y_train)\n        train_losses[epoch] = train_loss.data\n\n        # forward full validation set\n        validate_output = forward(model, x_validate)\n        validate_loss = loss(validate_output, y_validate)\n        validate_losses[epoch] = validate_loss.data\n        \n        # calculate accuracy\n        validate_accuracy[epoch] = accuracy(validate_output, y_validate)\n\n        print(\n            f\"Epoch {epoch}, train loss {train_loss.data:.3f}, validate loss {validate_loss.data:.3f}, accuracy {(validate_accuracy[epoch]*100):.3f}\"\n        )\n\n    print(\"Training completed.\")\n    print(f\"Training loss: {train_losses[-1]:.3f}\")\n    print(f\"Validation loss: {validate_losses[-1]:.3f}\")\n    print(f\"Validation accuracy: {(validate_accuracy[-1]*100):.3f}%\")\n\n    return train_losses, validate_losses, validate_accuracy","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:58.87168Z","iopub.execute_input":"2023-12-25T07:03:58.872068Z","iopub.status.idle":"2023-12-25T07:03:58.88399Z","shell.execute_reply.started":"2023-12-25T07:03:58.872036Z","shell.execute_reply":"2023-12-25T07:03:58.883085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define number of input features, hidden layers, and output features\nnum_inputs = X_train.shape[1]\nnum_hidden = [100]\nnum_outputs = 1\n\n# Initialize the model\nmodel = init_model(num_inputs, num_hidden, num_outputs)\n\nprint(model)\nprint(f\"Parameters: {len(model.parameters())}\")\n\n# Define the training parameters\nbatch_size = 50 # None = full batch\nnum_epochs = 100\n\n# Train the model\ntrain_losses, validate_losses, validate_accuracy = train_model(\n    model,\n    (X_train, y_train),\n    (X_validate, y_validate),\n    num_epochs=num_epochs,\n    batch_size=batch_size,\n)\n\n# Plot the results\nplt.subplot(211)\nplt.ylabel('Accuracy')\nplt.plot(validate_accuracy, label='Accuracy')\n\nplt.subplot(212)\nplt.ylabel('Loss')\nplt.plot(train_losses, label='Training Loss')\nplt.plot(validate_losses, label='Validation Loss')\nplt.legend()\nplt.xlabel(\"Epoch\")","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:03:58.88624Z","iopub.execute_input":"2023-12-25T07:03:58.887486Z","iopub.status.idle":"2023-12-25T07:05:09.518928Z","shell.execute_reply.started":"2023-12-25T07:03:58.887434Z","shell.execute_reply":"2023-12-25T07:05:09.517584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Input features from the scaled test dataset\ntest_features = test_scaled_df.to_numpy()\n\n# Forward full test set\ntest_output = forward(model, test_features)\ntest_output_binary = [1 if x.data > 0.5 else 0 for x in test_output]\n\n# Create submission dataframe\nsubmission_df = pd.DataFrame(test_output_binary, index=test_df.index, columns=[\"Survived\"])\nsubmission_df.to_csv(\"submission.csv\")\n\nsubmission_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T07:05:09.52068Z","iopub.execute_input":"2023-12-25T07:05:09.521103Z","iopub.status.idle":"2023-12-25T07:05:36.046928Z","shell.execute_reply.started":"2023-12-25T07:05:09.521069Z","shell.execute_reply":"2023-12-25T07:05:36.045907Z"},"trusted":true},"execution_count":null,"outputs":[]}]}